---
layout: post
title: 机器学习问答
date: 2021-11-22 09:17 +0800
---

# 初学者解惑
##
# 基本概念
# 常见算法

## 逻辑回归
https://zhuanlan.zhihu.com/p/56900935

知识点:
1. 也就是说，输出 Y=1 的对数几率是由输入 x 的线性函数表示的模型，这就是逻辑回归模型。
2. 核心是对数几率与输入x的线性关系. ln(P(Y=1|x)/(1 - P(Y=1|1)) = wx + b
3. 逻辑回归是在线性回归的基础上加了一个 Sigmoid 函数（非线形）映射，使得逻辑回归称为了一个优秀的分类算法。本质上来说，两者都属于广义线性模型，但他们两个要解决的问题不一样，逻辑回归解决的是分类问题，输出的是离散值，线性回归解决的是回归问题，输出的连续值。
4.

优点:
1. 直接对分类的概率建模，无需实现假设数据分布，从而避免了假设分布不准确带来的问题（区别于生成式模型）；
2. 不仅可预测出类别，还能得到该预测的概率，这对一些利用概率辅助决策的任务很有用；
   1. 因为Y是一个越迁曲线，所以大部分值落在0和1这两个点上，0和1之间的情况非常少，所以大部分情况下就是一个0和1的分类。
3. 对数几率函数是任意阶可导的凸函数，有许多数值优化算法都可以求出最优解。

others:
1. 正则化是一个通用的算法和思想，所以会产生过拟合现象的算法都可以使用正则化来避免过拟合。
    在经验风险最小化的基础上（也就是训练误差最小化），尽可能采用简单的模型，可以有效提高泛化预测精度。如果模型过于复杂，变量值稍微有点变动，就会引起预测精度问题。正则化之所以有效，就是因为其降低了特征的权重，使得模型更为简单。
1.

正则化一般会采用 L1 范式或者 L2 范式，其形式分别为 [公式] 和 [公式] 。
### 与SVM
相同点：

1. 都是分类算法，本质上都是在找最佳分类超平面；
1. 都是监督学习算法；
1. 都是判别式模型，判别模型不关心数据是怎么生成的，它只关心数据之间的差别，然后用差别来简单对给定的一个数据进行分类；
1. 都可以增加不同的正则项。

不同点：

1. LR 是一个统计的方法，SVM 是一个几何的方法；
1. SVM 的处理方法是只考虑 Support Vectors，也就是和分类最相关的少数点去学习分类器。而逻辑回归通过非线性映射减小了离分类平面较远的点的权重，相对提升了与分类最相关的数据点的权重；
1. 损失函数不同：LR 的损失函数是交叉熵，SVM 的损失函数是 HingeLoss，这两个损失函数的目的都是增加对分类影响较大的数据点的权重，减少与分类关系较小的数据点的权重。对 HingeLoss 来说，其零区域对应的正是非支持向量的普通样本，从而所有的普通样本都不参与最终超平面的决定，这是支持向量机最大的优势所在，对训练样本数目的依赖大减少，而且提高了训练效率；
1. LR 是参数模型，SVM 是非参数模型，参数模型的前提是假设数据服从某一分布，该分布由一些参数确定（比如正太分布由均值和方差确定），在此基础上构建的模型称为参数模型；非参数模型对于总体的分布不做任何假设，只是知道总体是一个随机变量，其分布是存在的（分布中也可能存在参数），但是无法知道其分布的形式，更不知道分布的相关参数，只有在给定一些样本的条件下，能够依据非参数统计的方法进行推断。所以 LR 受数据分布影响，尤其是样本不均衡时影响很大，需要先做平衡，而 SVM 不直接依赖于分布；
1. LR 可以产生概率，SVM 不能；
1. LR 不依赖样本之间的距离，SVM 是基于距离的；
1. LR 相对来说模型更简单好理解，特别是大规模线性分类时并行计算比较方便。而 SVM 的理解和优化相对来说复杂一些，SVM 转化为对偶问题后，分类只需要计算与少数几个支持向量的距离，这个在进行复杂核函数计算时优势很明显，能够大大简化模型和计算。


## SVM
1. 通过核函数，把二维数据变成3维数据，然后在3维空间进行平面分割。是一个分类算法。
2.
