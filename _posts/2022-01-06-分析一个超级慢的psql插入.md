---
layout: post
title: 分析一个超级慢的psql插入
date: 2022-01-06 09:54 +0800
---

项目组做一个日志分析的工具，每天会导入大概3万条日志，插入到psql. 导入后再做分析，分给对应的人。

日志导入这一块一直是个头疼的事情。先描述基本数据:
1. 每次导入的日志总量大概在3w条. json文件大小在1-2G。
   1. 2G日志压缩后16M上传到百度网盘了,链接: https://pan.baidu.com/s/12vO-aGGYfEGySDCe20V-oA  密码: ok7s
2. 每次导入时内存占用在十多个G。用的MacBook Pro, 32G内存.
3. 导入时通过批量插入的方式完成, 一次插入1000条记录, 总共3万条记录.
4. 有一张日志表，用来关联日志和分析结果的，一对一的关系，3万条记录。
5. 为了后续分析方便，每条日志都会把原始数据original_json保存到一个字段，jsonb格式,大小大概在50k-70k左右, 小的有11k的, 大的有到280K的. 这个字段早期就创建了。
6. 为了后续分析方便，每条日志都会把stacktrace保存到一个字段，jsonb格式,大小大概在50k左右, 大的有到120K的.
7. 导入完成后会做分析，产生分析结果。分析结果主要是需要上报的bug。bug数量不多，总数最多也就几百吧。
8. 每次导入前会把以前的日志清除。但是保留分析结果。
9. 最开始导入的时候大概20多分钟可以搞定。
10. 导入几次以后就变成800多分钟才能搞定了。
11. 再后来变成1400多分钟，一整天。数据量没有变过。
12. 确定不是后端代码的问题。把插入语句注释掉以后所有流程跑完只需要几分钟.


前期分析:
1. 内存应该是够的。不排除内存交换到磁盘太多导致的速度问题。
2. CPU够。
3. 速度慢主要在批量插入那一块。
4. 所以下一步主要方向先分析psql性能日志.


# postgres 日志分析
用pgbadger分析日志
1. 安装pgbadger: `brew install pgbadger`
1. 分析日志: `pgbadger '/Users/xxxx/Library/Application Support/Postgres/var-12/postgresql.log'`
2. 获取分析报告: `open ./out.html`
   1. parsed events: 2878958
   2. 啥也没有看出来。

现在不知道怎么搞了。

试试进psql进行单条插入和count查询，看看速度。

# 解决方法
把batch insert 1000 改成10条每batch, 时间瞬间降下来了，同样的数据71分钟导入完成.

具体原因未知。留一个历史疑案吧。


# refs
1. [记一次PostgreSQL数据库超级慢故障排除](https://www.jianshu.com/p/ac4cb5a1f842)
